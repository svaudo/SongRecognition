{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d351cde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "from microphone import record_audio\n",
    "from typing import Tuple\n",
    "import librosa\n",
    "from random import randint\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "# For peak finding:\n",
    "from scipy.ndimage.filters import maximum_filter\n",
    "from scipy.ndimage.morphology import generate_binary_structure, binary_erosion\n",
    "from scipy.ndimage.morphology import iterate_structure\n",
    "from typing import Tuple, Callable, List\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1382ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create functions for converting all variety of audio recordings into a NumPy-array of digital samples.\n",
    "\n",
    "def from_mp3(local_song_path: str, length: float, sr: int):\n",
    "    #Gets MP3 from disk\n",
    "    samples, sample_rate = librosa.load(local_song_path, sr=sr, mono=True, duration=length)\n",
    "    \n",
    "    #Turns MP3 into .npy file and saves to disk\n",
    "    samples = np.hstack([np.frombuffer(i, np.int16) for i in samples])\n",
    "    array = np.hstack((sample_rate, samples)) #sample rate is first\n",
    "    \n",
    "    return array\n",
    "\n",
    "def from_recording(length: float, sr: int):\n",
    "    samples, sample_rate = record_audio(length)\n",
    "    \n",
    "    #Turns MP3 into .npy file and saves to disk\n",
    "    samples = np.hstack([np.frombuffer(i, np.int16) for i in samples])\n",
    "    array = np.hstack((sample_rate, samples)) #sample rate is first\n",
    "    \n",
    "    return array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52396281",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import njit\n",
    "\n",
    "# `@njit` \"decorates\" the `_peaks` function. This tells Numba to\n",
    "# compile this function using the \"low level virtual machine\" (LLVM)\n",
    "# compiler. The resulting object is a Python function that, when called,\n",
    "# executes optimized machine code instead of the Python code\n",
    "# \n",
    "# The code used in _peaks adheres strictly to the subset of Python and\n",
    "# NumPy that is supported by Numba's jit. This is a requirement in order\n",
    "# for Numba to know how to compile this function to more efficient\n",
    "# instructions for the machine to execute\n",
    "\n",
    "@njit\n",
    "\n",
    "def _peaks(\n",
    "    data_2d: np.ndarray, rows: np.ndarray, cols: np.ndarray, amp_min: float\n",
    ") -> List[Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    A Numba-optimized 2-D peak-finding algorithm.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_2d : numpy.ndarray, shape-(H, W)\n",
    "        The 2D array of data in which local peaks will be detected.\n",
    "\n",
    "    rows : numpy.ndarray, shape-(N,)\n",
    "        The 0-centered row indices of the local neighborhood mask\n",
    "    \n",
    "    cols : numpy.ndarray, shape-(N,)\n",
    "        The 0-centered column indices of the local neighborhood mask\n",
    "        \n",
    "    amp_min : float\n",
    "        All amplitudes at and below this value are excluded from being local \n",
    "        peaks.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    List[Tuple[int, int]]\n",
    "        (row, col) index pair for each local peak location. \n",
    "    \"\"\"\n",
    "    \n",
    "    peaks = []  # stores the (row, col) locations of all the local peaks\n",
    "\n",
    "    # Iterate over the 2-D data in col-major order\n",
    "    # we want to see if there is a local peak located at\n",
    "    # row=r, col=c\n",
    "    for c, r in np.ndindex(*data_2d.shape[::-1]):\n",
    "        if data_2d[r, c] <= amp_min:\n",
    "            # The amplitude falls beneath the minimum threshold\n",
    "            # thus this can't be a peak.\n",
    "            continue\n",
    "        \n",
    "        # Iterating over the neighborhood centered on (r, c)\n",
    "        # dr: displacement from r\n",
    "        # dc: discplacement from c\n",
    "        for dr, dc in zip(rows, cols):\n",
    "            if dr == 0 and dc == 0:\n",
    "                # This would compare (r, c) with itself.. skip!\n",
    "                continue\n",
    "\n",
    "            if not (0 <= r + dr < data_2d.shape[0]):\n",
    "                # neighbor falls outside of boundary\n",
    "                continue\n",
    "\n",
    "            # mirror over array boundary\n",
    "            if not (0 <= c + dc < data_2d.shape[1]):\n",
    "                # neighbor falls outside of boundary\n",
    "                continue\n",
    "\n",
    "            if data_2d[r, c] < data_2d[r + dr, c + dc]:\n",
    "                # One of the amplitudes within the neighborhood\n",
    "                # is larger, thus data_2d[r, c] cannot be a peak\n",
    "                break\n",
    "        else:\n",
    "            # if we did not break from the for-loop then (r, c) is a peak\n",
    "            peaks.append((r, c))\n",
    "    return peaks\n",
    "\n",
    "# `local_peak_locations` is responsible for taking in the boolean mask `neighborhood`\n",
    "# and converting it to a form that can be used by `_peaks`. This \"outer\" code is \n",
    "# not compatible with Numba which is why we end up using two functions:\n",
    "# `local_peak_locations` does some initial pre-processing that is not compatible with\n",
    "# Numba, and then it calls `_peaks` which contains all of the jit-compatible code\n",
    "def local_peak_locations(data_2d: np.ndarray, neighborhood: np.ndarray, amp_min: float):\n",
    "    \"\"\"\n",
    "    Defines a local neighborhood and finds the local peaks\n",
    "    in the spectrogram, which must be larger than the specified `amp_min`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_2d : numpy.ndarray, shape-(H, W)\n",
    "        The 2D array of data in which local peaks will be detected\n",
    "    \n",
    "    neighborhood : numpy.ndarray, shape-(h, w)\n",
    "        A boolean mask indicating the \"neighborhood\" in which each\n",
    "        datum will be assessed to determine whether or not it is\n",
    "        a local peak. h and w must be odd-valued numbers\n",
    "        \n",
    "    amp_min : float\n",
    "        All amplitudes at and below this value are excluded from being local \n",
    "        peaks.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    List[Tuple[int, int]]\n",
    "        (row, col) index pair for each local peak location.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    Neighborhoods that overlap with the boundary are mirrored across the boundary.\n",
    "    \n",
    "    The local peaks are returned in column-major order.\n",
    "    \"\"\"\n",
    "    \n",
    "    rows, cols = np.where(neighborhood)\n",
    "    assert neighborhood.shape[0] % 2 == 1\n",
    "    assert neighborhood.shape[1] % 2 == 1\n",
    "\n",
    "    # center neighborhood indices around center of neighborhood\n",
    "    rows -= neighborhood.shape[0] // 2\n",
    "    cols -= neighborhood.shape[1] // 2\n",
    "\n",
    "    return _peaks(data_2d, rows, cols, amp_min=amp_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "76d87e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_peaks_mask(data: np.ndarray, cutoff: float) -> np.ndarray:\n",
    "    \"\"\"Find local peaks in a 2D array of data.\n",
    "\n",
    "    Parameters:\n",
    "    data : numpy.ndarray, shape-(H, W)\n",
    "    cutoff : float\n",
    "         A threshold value that distinguishes background from foreground\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Binary indicator, of the same shape as `data`. The value of\n",
    "    1 indicates a local peak.\"\"\"\n",
    "    \n",
    "    # Generate a rank-2, connectivity-2 binary mask\n",
    "    r2c2 = generate_binary_structure(2, 2)\n",
    "\n",
    "\n",
    "    # Use that neighborhood to find the local peaks in `data`.\n",
    "    # Pass `cutoff` as `amp_min` to `local_peak_locations`.\n",
    "    peak_locations = local_peak_locations(data, r2c2, cutoff)\n",
    "    \n",
    "\n",
    "    # Turns the list of (row, col) peak locations into a shape-(N_peak, 2) array\n",
    "    # Save the result to the variable `peak_locations`\n",
    "    peak_locations = np.array(peak_locations)\n",
    "\n",
    "    # create a mask of zeros with the same shape as `data`\n",
    "    mask = np.zeros(data.shape, dtype=bool)\n",
    "\n",
    "    # populate the local peaks with `1`\n",
    "    mask[peak_locations[:, 0], peak_locations[:, 1]] = 1\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7d0f33a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def peaks_to_fingerprints (local_peaks: np.ndarray, n=15):\n",
    "    \"\"\"Takes in the array local_peaks of freq, time for each peak\n",
    "    for all of the local peaks in a section of a song and takes in\n",
    "    fanout value n.\n",
    "    \n",
    "    It will return fingerprints for each peak entered in an ndarray \n",
    "    \"\"\"\n",
    "    \n",
    "    pairs = form_peak_pairs(local_peaks)\n",
    "    peaks = points_to_data(pairs)\n",
    "\n",
    "    fingerprint = np.array([peaks_to_fanout(\n",
    "        peaks[\n",
    "            np.where(np.multiply(pairs[:,0,0] == i[0], pairs[:,0,1] == i[1]))],\n",
    "        n) for i in local_peaks if np.size(\n",
    "        np.where(np.multiply(pairs[:,0,0] == i[0], pairs[:,0,1] == i[1])))>= n])\n",
    "    \n",
    "    ### np.where(np.multiply(pairs[:,0,0] == i[0], pairs[:,0,1] == i[1])) is the index of the peak pair data (row containing it) for each peak i\n",
    "    ### comparing the first and second items of each peak with those of peak i\n",
    "    \n",
    "    \"\"\"\n",
    "    # GET RID OF FOOR LOOP\n",
    "    for i in local_peaks: # each peak...\n",
    "        indices = np.array(np.where(np.multiply(pairs[:,0,0] == i[0], pairs[:,0,1] == i[1]))) # index of peak pair data (row containing it) where first peak is peak i (can ignore the second peak since we are only looking forward)\n",
    "        \n",
    "        ### indices[i] --> (row) index of peak-pair P the array peaks (peaks[indices[i], : ] --> original data for peak-pair P)\n",
    "        \n",
    "        selected = peaks[indices, : ]\n",
    "        \n",
    "        if len(selected) >= n: # only going to add it if it's long enough\n",
    "            fanout = peaks_to_fanout(selected, n)\n",
    "    \"\"\"\n",
    "        \n",
    "    ### return array of the fingerprints for each peak\n",
    "    return fingerprint\n",
    "\n",
    "def form_peak_pairs(local_peaks: np.ndarray):\n",
    "    \"\"\"Takes in the array local_peaks of any number of rows and two columns freq and time\n",
    "    for each of the local peaks and returns an array of each peak-pair from these peaks\n",
    "    \"\"\"\n",
    "    \n",
    "    pairs = np.array([(peak1,peak2) for peak2 in local_peaks for peak1 in local_peaks if peak1[1] < peak2[1]])    \n",
    "    \n",
    "    return pairs\n",
    "\n",
    "def points_to_data(points: np.ndarray):\n",
    "    \"\"\"Takes in an array of freq and time data for each peak in each peak-pair\n",
    "    (where the first listed is one that occurs first) and returns a 2d array\n",
    "    which contains three columns and number of rows equivalent to the number of \n",
    "    peak-pairs entered\n",
    "        Col 1 --> frequency at point 1 (point 1 must occur first)\n",
    "        Col 2 --> frequency at point 2\n",
    "        Col 3 --> time elapsed between points\n",
    "    \"\"\"\n",
    "    data = np.array([[pair[0,0], pair[1,0], pair[1,1]-pair[0,1]] for pair in points])\n",
    "    return data\n",
    "\n",
    "def peaks_to_fanout(selected_peaks: np.ndarray, n):\n",
    "    \"\"\"Takes in a 2d array selected_peaks of selected peak-pair\n",
    "    data in the same formatting as the full array, and containing\n",
    "    all of the peak-pairs with a particular peak as peak 1. Also\n",
    "    takes in the fanout value n.\n",
    "    \n",
    "    Returns the fanout for this peak\n",
    "\n",
    "                                  [[fi, fi+1, delta_t(i,i+1)]\n",
    "                                   [fi, fi+2, delta_t(i,i+2)],\n",
    "        fanout (for peak i)  =               ...,\n",
    "                                   [fi, fi+n, delta_t(i,i+n)]]    \n",
    "    \"\"\"\n",
    "\n",
    "    delta_fs = selected_peaks[:,1]-selected_peaks[:,0] # array of change in frequencies for our peak pairs\n",
    "    delta_ts = selected_peaks[:,2] # array of change in times for our peak pairs\n",
    "\n",
    "    ### for each index i,\n",
    "    ###     selected_peaks[i] --> peak-pair P data (this is just a segment of the original data; just the peak-pairs contianing our particular peak)\n",
    "    ###     delta_fs[i]       --> change in frequency for peak-pair P\n",
    "    ###     delta_ts[i]       --> change in time for peak-pair P\n",
    "\n",
    "\n",
    "    # an array of indices pointing towards our selected peak-pair data\n",
    "    # (selected_peaks_sorted[i] --> index of i'th closest peak (index pointing towards our selected peak-pair data arrays))\n",
    "    selected_peaks_sorted = sort_peaks(delta_fs, delta_ts) \n",
    "\n",
    "    #selected_peaks_sorted[0:n] are the indices of the peaks within the fanout\n",
    "    fanout_inds = selected_peaks_sorted[0:n]\n",
    "    fanout_peaks = selected_peaks[fanout_inds]\n",
    "\n",
    "    ###    fanout for this peak = an array of [[fi, fi+1, delta_t(i,i+1)]\n",
    "    ###                                        [fi, fi+2, delta_t(i,i+2)],\n",
    "    ###                                                   ...,\n",
    "    ###                                        [fi, fi+n, delta_t(i,i+n)]]\n",
    "    \n",
    "    fanout = fanout_peaks[:,0:3]\n",
    "    return fanout\n",
    "\n",
    "def sort_peaks (delta_fs: np.ndarray, delta_ts: np.ndarray):\n",
    "    \"\"\"Takes in two 1 dimensional nd arrays delta_fs and delta_ts\n",
    "    of the same length which both point towards data from the same\n",
    "    selected peak-pairs and for which the value of each at each same\n",
    "    index point to the same individual peak-pair\n",
    "        - this means that for each index i, delta_fs[i] and delta_ts[i]\n",
    "          correspond to the same peak-pair\n",
    "        - the delta_f and delta_t values should be from all of the\n",
    "          peak-pairs for which a particular peak is the first peak in\n",
    "          the pair\n",
    "    \n",
    "    It returns an nd array where the values of which represent the\n",
    "    indices of delta_ts, delta_fs, and other arrays which point to the\n",
    "    same data in the same order sorted by time then frequency (eg. the\n",
    "    first value of the output array is the index of the smallest\n",
    "    delta_t, second is the next smallest, etc. (where identical delta_ts\n",
    "    will be ordered by smallest delta_f to greatest))\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    ### get indices sorted based on delta_t values of our selected data (sorted_t)\n",
    "    sorted_t = np.argsort(delta_ts)\n",
    "\n",
    "    # we need to make sure that any duplicate times are sorted by frequency:\n",
    "\n",
    "    #finding the time values\n",
    "    time_counts = Counter(delta_ts)\n",
    "    arr_counts = np.array(list(time_counts.items())) # an array of the items in the Counter time_counts\n",
    "    duplicates_ind = np.where(arr_counts[:,1] >1) # finding the indexes of where there are more than one of each time\n",
    "    if len(duplicates_ind) > 0: # only continue to sort if there are duplicates\n",
    "        duplicates_times = arr_counts[:, 0][duplicates_ind] #finding the time values based on the indices in the counter\n",
    "\n",
    "        # list of an array of indices of each duplicate time\n",
    "        # (indices are the indices pointing towards data in delta_fs, delta_ts, and t_sorted, NOT the values in t_sorted which will be used as indices (to get those do t_sorted[unsorted_by_freq]))\n",
    "        unsorted_by_freq = [list(np.where(delta_ts == time)[0]) for time in duplicates_times]\n",
    "\n",
    "        # list of an array containing the new index values pointed to by each array of indices in unsorted_by_freq, sorted by frequency\n",
    "        # (these values are the index values to put into the array of indices)\n",
    "        # each item = reorder of the values of sorted_t by sorting by frequency for each index_arr in the unsorted_by_freq lsit\n",
    "        sorted_by_freq = [list(sorted_t[index_arr][np.argsort(delta_fs[sorted_t[index_arr]])]) for index_arr in unsorted_by_freq]                \n",
    "        # put back in:\n",
    "        ## 'flattening' the lists so as to be able to index with them\n",
    "        flat_unsorted = []\n",
    "        flat_sorted = []\n",
    "        [[flat_unsorted.append(inner_vals) for inner_vals in value] for value in unsorted_by_freq]\n",
    "        [[flat_sorted.append(inner_vals) for inner_vals in value] for value in sorted_by_freq]\n",
    "        ## putting these into sorted\n",
    "        sorted_fully = sorted_t.copy()\n",
    "        sorted_fully[flat_unsorted] = flat_sorted\n",
    "        \n",
    "        return sorted_fully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9031a286",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Databases\n",
    "database_names = {}\n",
    "database_fingerprints = {}\n",
    "#add functions\n",
    "def add_to_DBN(song_name: str, artist: str):\n",
    "    database_names[len(database)+1] = (song_name,artist)\n",
    "    \n",
    "    \n",
    "def add_to_DBF(fingerprint_arr: tuple, time_occured: float,  song_ID: str):\n",
    "    if (fingerprint_arr in database_fingerprints.keys()):\n",
    "        temp_tuple = database_fingerprints[fingerprint_arr] + (song_ID,time_occured)\n",
    "        database_fingerprints[fingerprint_arr] = temp_tuple\n",
    "    else:\n",
    "        database_fingerprints[fingerprint_arr] = (song_ID, time_occured)\n",
    "        \n",
    "def del_id(ex_dict, id_n):\n",
    "    del ex_dict[id_n]\n",
    "\n",
    "def get_all(ex_dict: dict):\n",
    "    # ex_dict is the dictionary which stores a tuple of songs and artists as the value of keys which are ids.\n",
    "    return [ex_dict[i] for i in range(1,len(ex_dict)+1)]\n",
    "    # i refers to the id number, range begins at 1 and len is added by 1 because id begins (here at least) with 1, not 0. \n",
    "\n",
    "def get_song_names(ex_dict: dict):\n",
    "    return [ex_dict[i][0] for i in range(1, len(ex_dict)+1)]\n",
    "\n",
    "def get_artists(ex_dict: dict):\n",
    "    return [ex_dict[i][1] for i in range(1, len(ex_dict)+1)]\n",
    "\n",
    "#add_to_DBF(tuple(np.array([0,0,0,1])) ,0, 1)\n",
    "#add_to_DBF(tuple(np.array([0,0,0,0])) ,1, 1)\n",
    "#add_to_DBF(tuple(np.array([0,0,0,0])) ,2, 1)\n",
    "#del_id(database_fingerprints, tuple(np.array([0,0,0,0])))\n",
    "#database_fingerprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d7008500",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clips\n",
    "def short_clip(percent, data): # percent refers to the percentage of the original file the new file should be\n",
    "    # example: a 15 second clip with a percent of 10% would return files that are 1.5 seconds long.\n",
    "    # Where in the original clip the function pulls from is randomly chosen.\n",
    "    percent /= 100\n",
    "    start = randint(0, len(data)-int(len(data)*percent)) # Chooses a random starting point for the new file, avoids out of bounds errors\n",
    "    print(\"start ind is\", start, \"of\", len(data)-int(len(data)*percent), \"possible inds\")\n",
    "    return data[start:start+int(len(data)*percent)]\n",
    "\n",
    "def multiple_clips(percent, data, num_clips):\n",
    "    clips = [] # nd array would also work\n",
    "    for i in range(num_clips):\n",
    "        clips.append(short_clip(15, data))\n",
    "    return clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eaa0ccc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Andrew Lloyd Webber - The Music of the Night.mp3',\n",
       " 'Daniela Mercury - O Canto Da Cidade.mp3',\n",
       " 'David Bowie â€“ Space Oddity.mp3',\n",
       " 'Imagine Dragons - I Bet My Life.mp3',\n",
       " 'Jimi Hendrix - Little Wing.mp3',\n",
       " 'Lewis Capaldi - Someone You Loved.mp3',\n",
       " 'Mat Kearney - I Dont Really Care.mp3',\n",
       " 'Rick Astley - Never Gonna Give You Up.mp3']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('./MusicMP3s')[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5442f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
